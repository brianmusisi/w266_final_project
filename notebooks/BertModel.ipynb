{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 18:01:35.555200 140215057229632 deprecation_wrapper.py:119] From /home/brianmusisi/anaconda3/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tibit Communications Raises $20M in Series B F...</td>\n",
       "      <td>Tibit Communications, Inc., a Petaluma, CA-bas...</td>\n",
       "      <td>http://www.finsmes.com/2019/04/tibit-communica...</td>\n",
       "      <td>FinsmesUSA</td>\n",
       "      <td>Funding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter blames human error after blocking a Ne...</td>\n",
       "      <td>Over the holiday weekend, The New York Times f...</td>\n",
       "      <td>https://techcrunch.com/2017/11/27/twitter-blam...</td>\n",
       "      <td>techcrunch</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SimplyCook Raises £4.5M in Series A Funding\\n</td>\n",
       "      <td>SimplyCook, a London, UK-based recipe kit serv...</td>\n",
       "      <td>http://www.finsmes.com/2019/01/simplycook-rais...</td>\n",
       "      <td>FinsmesUK</td>\n",
       "      <td>Funding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moogsoft Secures $40M in Series D Funding\\n</td>\n",
       "      <td>Moogsoft, a San Francisco, CA-based provider o...</td>\n",
       "      <td>http://www.finsmes.com/2018/03/moogsoft-secure...</td>\n",
       "      <td>FinsmesUSA</td>\n",
       "      <td>Funding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zeta Global acquires commenting service†Disqus</td>\n",
       "      <td>A source close to the two companies tells us t...</td>\n",
       "      <td>https://techcrunch.com/2017/12/05/zeta-global-...</td>\n",
       "      <td>techcrunch</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Tibit Communications Raises $20M in Series B F...   \n",
       "1  Twitter blames human error after blocking a Ne...   \n",
       "2      SimplyCook Raises £4.5M in Series A Funding\\n   \n",
       "3        Moogsoft Secures $40M in Series D Funding\\n   \n",
       "4     Zeta Global acquires commenting service†Disqus   \n",
       "\n",
       "                                             content  \\\n",
       "0  Tibit Communications, Inc., a Petaluma, CA-bas...   \n",
       "1  Over the holiday weekend, The New York Times f...   \n",
       "2  SimplyCook, a London, UK-based recipe kit serv...   \n",
       "3  Moogsoft, a San Francisco, CA-based provider o...   \n",
       "4  A source close to the two companies tells us t...   \n",
       "\n",
       "                                                link      source    class  \n",
       "0  http://www.finsmes.com/2019/04/tibit-communica...  FinsmesUSA  Funding  \n",
       "1  https://techcrunch.com/2017/11/27/twitter-blam...  techcrunch    Other  \n",
       "2  http://www.finsmes.com/2019/01/simplycook-rais...   FinsmesUK  Funding  \n",
       "3  http://www.finsmes.com/2018/03/moogsoft-secure...  FinsmesUSA  Funding  \n",
       "4  https://techcrunch.com/2017/12/05/zeta-global-...  techcrunch    Other  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset/data/articles_dataset.csv')\n",
    "data = data[~data['content'].isnull()]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44031, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tibit Communications Raises $20M in Series B F...</td>\n",
       "      <td>Tibit Communications, Inc., a Petaluma, CA-bas...</td>\n",
       "      <td>http://www.finsmes.com/2019/04/tibit-communica...</td>\n",
       "      <td>FinsmesUSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter blames human error after blocking a Ne...</td>\n",
       "      <td>Over the holiday weekend, The New York Times f...</td>\n",
       "      <td>https://techcrunch.com/2017/11/27/twitter-blam...</td>\n",
       "      <td>techcrunch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SimplyCook Raises £4.5M in Series A Funding\\n</td>\n",
       "      <td>SimplyCook, a London, UK-based recipe kit serv...</td>\n",
       "      <td>http://www.finsmes.com/2019/01/simplycook-rais...</td>\n",
       "      <td>FinsmesUK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moogsoft Secures $40M in Series D Funding\\n</td>\n",
       "      <td>Moogsoft, a San Francisco, CA-based provider o...</td>\n",
       "      <td>http://www.finsmes.com/2018/03/moogsoft-secure...</td>\n",
       "      <td>FinsmesUSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zeta Global acquires commenting service†Disqus</td>\n",
       "      <td>A source close to the two companies tells us t...</td>\n",
       "      <td>https://techcrunch.com/2017/12/05/zeta-global-...</td>\n",
       "      <td>techcrunch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Tibit Communications Raises $20M in Series B F...   \n",
       "1  Twitter blames human error after blocking a Ne...   \n",
       "2      SimplyCook Raises £4.5M in Series A Funding\\n   \n",
       "3        Moogsoft Secures $40M in Series D Funding\\n   \n",
       "4     Zeta Global acquires commenting service†Disqus   \n",
       "\n",
       "                                             content  \\\n",
       "0  Tibit Communications, Inc., a Petaluma, CA-bas...   \n",
       "1  Over the holiday weekend, The New York Times f...   \n",
       "2  SimplyCook, a London, UK-based recipe kit serv...   \n",
       "3  Moogsoft, a San Francisco, CA-based provider o...   \n",
       "4  A source close to the two companies tells us t...   \n",
       "\n",
       "                                                link      source  class  \n",
       "0  http://www.finsmes.com/2019/04/tibit-communica...  FinsmesUSA      1  \n",
       "1  https://techcrunch.com/2017/11/27/twitter-blam...  techcrunch      0  \n",
       "2  http://www.finsmes.com/2019/01/simplycook-rais...   FinsmesUK      1  \n",
       "3  http://www.finsmes.com/2018/03/moogsoft-secure...  FinsmesUSA      1  \n",
       "4  https://techcrunch.com/2017/12/05/zeta-global-...  techcrunch      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'] = data['class'].map({'Funding':1, 'Other':0})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tibit Communications Raises $20M in Series B F...</td>\n",
       "      <td>Tibit Communications, Inc., a Petaluma, CA-bas...</td>\n",
       "      <td>http://www.finsmes.com/2019/04/tibit-communica...</td>\n",
       "      <td>FinsmesUSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter blames human error after blocking a Ne...</td>\n",
       "      <td>Over the holiday weekend, The New York Times f...</td>\n",
       "      <td>https://techcrunch.com/2017/11/27/twitter-blam...</td>\n",
       "      <td>techcrunch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SimplyCook Raises £4.5M in Series A Funding\\n</td>\n",
       "      <td>SimplyCook, a London, UK-based recipe kit serv...</td>\n",
       "      <td>http://www.finsmes.com/2019/01/simplycook-rais...</td>\n",
       "      <td>FinsmesUK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moogsoft Secures $40M in Series D Funding\\n</td>\n",
       "      <td>Moogsoft, a San Francisco, CA-based provider o...</td>\n",
       "      <td>http://www.finsmes.com/2018/03/moogsoft-secure...</td>\n",
       "      <td>FinsmesUSA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zeta Global acquires commenting service†Disqus</td>\n",
       "      <td>A source close to the two companies tells us t...</td>\n",
       "      <td>https://techcrunch.com/2017/12/05/zeta-global-...</td>\n",
       "      <td>techcrunch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Tibit Communications Raises $20M in Series B F...   \n",
       "1  Twitter blames human error after blocking a Ne...   \n",
       "2      SimplyCook Raises £4.5M in Series A Funding\\n   \n",
       "3        Moogsoft Secures $40M in Series D Funding\\n   \n",
       "4     Zeta Global acquires commenting service†Disqus   \n",
       "\n",
       "                                             content  \\\n",
       "0  Tibit Communications, Inc., a Petaluma, CA-bas...   \n",
       "1  Over the holiday weekend, The New York Times f...   \n",
       "2  SimplyCook, a London, UK-based recipe kit serv...   \n",
       "3  Moogsoft, a San Francisco, CA-based provider o...   \n",
       "4  A source close to the two companies tells us t...   \n",
       "\n",
       "                                                link      source  class  \n",
       "0  http://www.finsmes.com/2019/04/tibit-communica...  FinsmesUSA      1  \n",
       "1  https://techcrunch.com/2017/11/27/twitter-blam...  techcrunch      0  \n",
       "2  http://www.finsmes.com/2019/01/simplycook-rais...   FinsmesUK      1  \n",
       "3  http://www.finsmes.com/2018/03/moogsoft-secure...  FinsmesUSA      1  \n",
       "4  https://techcrunch.com/2017/12/05/zeta-global-...  techcrunch      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = round(0.8 * data.shape[0])\n",
    "dev_end = train_size + round(0.1 * data.shape[0])\n",
    "\n",
    "train_df = data.iloc[:train_size, :]\n",
    "dev_df = data.iloc[train_size : dev_end, :]\n",
    "test_df = data.iloc[dev_end :]\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 18:02:30.992128 140215057229632 deprecation_wrapper.py:119] From /home/brianmusisi/anaconda3/lib/python3.7/site-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create input examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_examples(df):\n",
    "  examples = df.apply(lambda row : run_classifier.InputExample(guid=None, text_a=row['content'], text_b=None, label= row['class']), axis=1)\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = create_input_examples(train_df)\n",
    "dev_examples = create_input_examples(dev_df)\n",
    "test_examples = create_input_examples(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting examples to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_features(example, max_len, tokenizer):\n",
    "  \n",
    "  example_tokens = tokenizer.tokenize(example.text_a)\n",
    "  \n",
    "  if len(example_tokens)> max_len -2:\n",
    "    example_tokens = example_tokens[: (max_len -2)]\n",
    "   \n",
    "  tokens = ['[CLS]']\n",
    "  tokens = tokens + example_tokens + ['[SEP]']\n",
    "  tokens=  tokens + ['[PAD]'] * (max_len - len(tokens))\n",
    "  \n",
    "  segment_ids = [0 for token in tokens]\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "  input_masks = [1] * (len(example_tokens) + 1) + [0] * (max_len - len(example_tokens) -1)\n",
    "  \n",
    "  return input_ids, input_masks, segment_ids, example.label\n",
    "\n",
    "\n",
    "def convert_example_list_to_features(tokenizer, examples, max_len):\n",
    "  input_ids = []\n",
    "  input_masks = []\n",
    "  segment_ids = []\n",
    "  labels = []\n",
    "  \n",
    "  for example in examples:\n",
    "    input_id, input_mask, segment_id, label = convert_example_to_features(example, max_len, tokenizer)\n",
    "    input_ids.append(input_id)\n",
    "    input_masks.append(input_mask)\n",
    "    segment_ids.append(segment_id)\n",
    "    labels.append(label)\n",
    "  \n",
    "  return np.array(input_ids), np.array(input_masks), np.array(segment_ids), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels) = convert_example_list_to_features(tokenizer, train_examples.tolist(), max_len=256)\n",
    "(dev_input_ids, dev_input_masks, dev_segment_ids, dev_labels) = convert_example_list_to_features(tokenizer, dev_examples.tolist(), max_len=256)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels) = convert_example_list_to_features(tokenizer, test_examples.tolist(), max_len=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First create BertLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\",\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "        trainable_vars = self.bert.variables\n",
    "        \n",
    "        # Remove unused layers\n",
    "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "        \n",
    "        # Select how many layers to fine tune\n",
    "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "        \n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "        \n",
    "        # Add non-trainable weights\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        \n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "            \"pooled_output\"\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0802 18:19:40.807928 140215057229632 deprecation.py:323] From /home/brianmusisi/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_2 (BertLayer)        (None, 768)          108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      bert_layer_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 109,128,517\n",
      "Trainable params: 6,102,273\n",
      "Non-trainable params: 103,026,244\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "in_id = tf.keras.layers.Input(shape=(max_len,), name=\"input_ids\")\n",
    "in_mask = tf.keras.layers.Input(shape=(max_len,), name=\"input_masks\")\n",
    "in_segment = tf.keras.layers.Input(shape=(max_len,), name=\"segment_ids\")\n",
    "bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "# Instantiate the custom Bert Layer defined above\n",
    "bert_output = BertLayer(n_fine_tune_layers=10)(bert_inputs)\n",
    "\n",
    "# Build the rest of the classifier \n",
    "dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start session and initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "def initialize_session(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n",
    "    \n",
    "initialize_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35225 samples, validate on 4403 samples\n",
      "Epoch 1/20\n",
      "35225/35225 [==============================] - 274s 8ms/sample - loss: 0.6392 - acc: 0.6285 - val_loss: 0.5606 - val_acc: 0.7434\n",
      "Epoch 2/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.3289 - acc: 0.8667 - val_loss: 0.3263 - val_acc: 0.8873\n",
      "Epoch 3/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.2608 - acc: 0.9013 - val_loss: 0.3236 - val_acc: 0.8776\n",
      "Epoch 4/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.2426 - acc: 0.9090 - val_loss: 0.2665 - val_acc: 0.9030\n",
      "Epoch 5/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.2350 - acc: 0.9120 - val_loss: 0.2610 - val_acc: 0.9023\n",
      "Epoch 6/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.2236 - acc: 0.9161 - val_loss: 0.2543 - val_acc: 0.9051\n",
      "Epoch 7/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.2126 - acc: 0.9214 - val_loss: 0.2531 - val_acc: 0.9096\n",
      "Epoch 8/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.2035 - acc: 0.9257 - val_loss: 0.2521 - val_acc: 0.9071\n",
      "Epoch 9/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.1981 - acc: 0.9269 - val_loss: 0.2539 - val_acc: 0.9071\n",
      "Epoch 10/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.1920 - acc: 0.9288 - val_loss: 0.2665 - val_acc: 0.8880\n",
      "Epoch 11/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.1878 - acc: 0.9301 - val_loss: 0.2766 - val_acc: 0.9046\n",
      "Epoch 12/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.1789 - acc: 0.9334 - val_loss: 0.2538 - val_acc: 0.9048\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.reset_states()\n",
    "history = model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([dev_input_ids, dev_input_masks, test_segment_ids], test_labels),\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on the test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4403/4403 [==============================] - 25s 6ms/sample - loss: 0.2213 - acc: 0.9219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22133106133381725, 0.9218714]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([test_input_ids, test_input_masks, test_segment_ids], test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy is: 0.9218714\n"
     ]
    }
   ],
   "source": [
    "print('Testing Accuracy is: {}'.format(0.9218714))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/Bert/BertModel_Dense256.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add more Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_4 (BertLayer)        (None, 768)          108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          196864      bert_layer_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          32896       dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            129         dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 109,161,285\n",
      "Trainable params: 6,135,041\n",
      "Non-trainable params: 103,026,244\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "in_id = tf.keras.layers.Input(shape=(max_len,), name=\"input_ids\")\n",
    "in_mask = tf.keras.layers.Input(shape=(max_len,), name=\"input_masks\")\n",
    "in_segment = tf.keras.layers.Input(shape=(max_len,), name=\"segment_ids\")\n",
    "bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "# Instantiate the custom Bert Layer defined above\n",
    "bert_output = BertLayer(n_fine_tune_layers=10)(bert_inputs)\n",
    "\n",
    "# Build the rest of the classifier \n",
    "dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "dense = tf.keras.layers.Dense(128, activation='relu')(dense)\n",
    "pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "initialize_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35225 samples, validate on 4403 samples\n",
      "Epoch 1/20\n",
      "35225/35225 [==============================] - 278s 8ms/sample - loss: 0.4242 - acc: 0.7887 - val_loss: 0.2925 - val_acc: 0.8901\n",
      "Epoch 2/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.2501 - acc: 0.9075 - val_loss: 0.2367 - val_acc: 0.9123\n",
      "Epoch 3/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.2356 - acc: 0.9128 - val_loss: 0.2253 - val_acc: 0.9187\n",
      "Epoch 4/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.2250 - acc: 0.9186 - val_loss: 0.2367 - val_acc: 0.9173\n",
      "Epoch 5/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.2196 - acc: 0.9193 - val_loss: 0.2460 - val_acc: 0.8971\n",
      "Epoch 6/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.2091 - acc: 0.9242 - val_loss: 0.2660 - val_acc: 0.9014\n",
      "Epoch 7/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.2029 - acc: 0.9272 - val_loss: 0.2233 - val_acc: 0.9212\n",
      "Epoch 8/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.1938 - acc: 0.9301 - val_loss: 0.2155 - val_acc: 0.9235\n",
      "Epoch 9/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.1861 - acc: 0.9340 - val_loss: 0.2248 - val_acc: 0.9196\n",
      "Epoch 10/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.1802 - acc: 0.9339 - val_loss: 0.2344 - val_acc: 0.9201\n",
      "Epoch 11/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.1701 - acc: 0.9391 - val_loss: 0.2263 - val_acc: 0.9196\n",
      "Epoch 12/20\n",
      "35225/35225 [==============================] - 272s 8ms/sample - loss: 0.1604 - acc: 0.9432 - val_loss: 0.2647 - val_acc: 0.9226\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.reset_states()\n",
    "history = model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([dev_input_ids, dev_input_masks, dev_segment_ids], dev_labels),\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4403/4403 [==============================] - 25s 6ms/sample - loss: 0.3247 - acc: 0.9087\n",
      "Testing Accuracy is: 0.9086986184120178\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate([test_input_ids, test_input_masks, test_segment_ids], test_labels)\n",
    "print('Testing Accuracy is: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/Bert/BertModel_Dense_256_128.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
