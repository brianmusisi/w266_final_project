{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jlMRjMiVebC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0803 06:55:51.877785 140067398080320 deprecation_wrapper.py:119] From /home/brianmusisi/anaconda3/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ydVFZYTRs4fM"
   },
   "outputs": [],
   "source": [
    "with open('ner_dataset/final_entity_sentences.csv', 'r') as f:\n",
    "  entity_sentences = f.readlines()\n",
    "  \n",
    "with open('ner_dataset/final_sentences.csv', 'r') as f:\n",
    "  sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBrpBZKUbIAk"
   },
   "outputs": [],
   "source": [
    "# Tensorflow hub path to BERT module\n",
    "bert_url = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
    "\n",
    "#Maximum token length\n",
    "max_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKLry34UcQcZ"
   },
   "source": [
    "#### Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "kuDOecgFcKcC",
    "outputId": "99fb0939-7610-4763-8803-835e6cb828ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0803 06:56:35.891704 140067398080320 deprecation_wrapper.py:119] From /home/brianmusisi/anaconda3/lib/python3.7/site-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_url)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFzaZNUvvXf-"
   },
   "source": [
    "#### Functions that convert to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4V2aOeTjcLy6"
   },
   "outputs": [],
   "source": [
    "def convert_example_to_features(example, max_len, tokenizer):\n",
    "  \n",
    "  example_tokens = tokenizer.tokenize(example)\n",
    "  \n",
    "  if len(example_tokens)> max_len -2:\n",
    "    example_tokens = example_tokens[: (max_len -2)]\n",
    "   \n",
    "  tokens = ['[CLS]']\n",
    "  tokens = tokens + example_tokens + ['[SEP]']\n",
    "  tokens=  tokens + ['[PAD]'] * (max_len - len(tokens))\n",
    "  \n",
    "  segment_ids = [0 for token in tokens]\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "  input_masks = [1] * (len(example_tokens) + 1) + [0] * (max_len - len(example_tokens) -1)\n",
    "  \n",
    "  return input_ids, input_masks, segment_ids\n",
    "\n",
    "\n",
    "def convert_example_list_to_features(tokenizer, examples, max_len):\n",
    "  input_ids = []\n",
    "  input_masks = []\n",
    "  segment_ids = []\n",
    "  labels = []\n",
    "  \n",
    "  for example in examples:\n",
    "    input_id, input_mask, segment_id = convert_example_to_features(example, max_len, tokenizer)\n",
    "    input_ids.append(input_id)\n",
    "    input_masks.append(input_mask)\n",
    "    segment_ids.append(segment_id)\n",
    "  \n",
    "  return np.array(input_ids), np.array(input_masks), np.array(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qp-ydT2OvgX4"
   },
   "outputs": [],
   "source": [
    "f = convert_example_list_to_features(tokenizer, sentences[4], 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ooP_brFUxKwG",
    "outputId": "ec2a5095-43bb-4276-e299-86af32be0eda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([101, 157, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JoCOQqF_xV2g"
   },
   "outputs": [],
   "source": [
    "def create_ner_tokens(entity_sentence, max_len):\n",
    "  ner_tokens = ['[nerCLS]']\n",
    "  entity_sentence = entity_sentence.replace('OTHER', 'O')\n",
    "  entity_sentence = re.sub('INVESTOR|STARTUP', 'B-org', entity_sentence)\n",
    "  tokens = entity_sentence.split()\n",
    "  \n",
    "  if len(tokens) > max_len -2:\n",
    "    tokens = tokens[: (max_len - 2)]\n",
    "    \n",
    "  ner_tokens = ner_tokens + tokens + ['[nerSEP]']\n",
    "  ner_tokens = ner_tokens + ['[nerPAD]'] * (max_len - len(ner_tokens))\n",
    "  return ner_tokens\n",
    "\n",
    "def get_tokens_for_list(entity_sentences, max_len):\n",
    "  ner_tokens_list = []\n",
    "  for sentence in entity_sentences:\n",
    "    ner_tokens = create_ner_tokens(sentence, max_len)\n",
    "    ner_tokens_list.append(ner_tokens)\n",
    "  \n",
    "  return np.array(ner_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hub_Klud5QEa",
    "outputId": "8a828eae-3a89-4c7b-b0e2-60895e58b86c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['[nerCLS]', 'O', 'O', 'B-org', '[nerSEP]', '[nerPAD]',\n",
       "        '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]']], dtype='<U8')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens_for_list(['OTHER OTHER STARTUP'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "As3-iIcN6pfy"
   },
   "source": [
    "### Convert Data into format BERT can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "1-qC4kpN5XTH",
    "outputId": "e6d71b52-d4e2-4841-c0ad-469f76c99d50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[nerCLS]', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org',\n",
       "       'O', 'O', 'O', 'O', '[nerSEP]', '[nerPAD]', '[nerPAD]', '[nerPAD]',\n",
       "       '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]',\n",
       "       '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]', '[nerPAD]',\n",
       "       '[nerPAD]'], dtype='<U8')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 30\n",
    "\n",
    "ner_tokens = get_tokens_for_list(entity_sentences, max_len)\n",
    "ner_tokens[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jc1ARUlJ9buf"
   },
   "outputs": [],
   "source": [
    "ner_id_dict = {'STARTUP':0, 'INVESTOR': 1, 'O': 2, '[nerCLS]': 3, '[nerSEP]': 4, '[nerPAD]':5 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ner_id_dict = {'B-org':0, 'O': 1, '[nerCLS]': 2, '[nerSEP]': 3, '[nerPAD]':4 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYIYERoR-b2B"
   },
   "outputs": [],
   "source": [
    "ner_labels =[]\n",
    "\n",
    "for sent in ner_tokens:\n",
    "  sent_ids = []\n",
    "  for token in sent:\n",
    "    sent_ids.append(new_ner_id_dict[token])\n",
    "  ner_labels.append(sent_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uPkJnKWs7dFm"
   },
   "outputs": [],
   "source": [
    "(input_ids, input_masks, segment_ids) = convert_example_list_to_features(tokenizer, sentences, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQsukXSf8A2Z"
   },
   "outputs": [],
   "source": [
    " ner_labels = np.array(ner_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOMA1ch1_JAG"
   },
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss function explicitly, filtering out 'extra inserted labels'\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length + 1) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns:  cost\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int32)),[-1])\n",
    "    \n",
    "    mask = (y_label < 2)   # This mask is used to remove all tokens that do not correspond to the original base text.\n",
    "\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)  # mask the labels\n",
    "    \n",
    "    y_flat_pred = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float32)),[-1, 5])\n",
    "    \n",
    "    y_flat_pred_masked = tf.boolean_mask(y_flat_pred, mask) # mask the predictions\n",
    "    \n",
    "    return tf.reduce_mean(sparse_categorical_crossentropy(y_label_masked, y_flat_pred_masked,from_logits=False ))\n",
    "\n",
    "\n",
    "def custom_acc_orig_tokens(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    calculate loss dfunction filtering out also the newly inserted labels\n",
    "    \n",
    "    y_true: Shape: (batch x (max_length) )\n",
    "    y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "    \n",
    "    returns: accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    #get labels and predictions\n",
    "    \n",
    "    y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "    \n",
    "    mask = (y_label < 2)\n",
    "    y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "    \n",
    "    y_predicted = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                    [-1, 5]), axis=1)\n",
    "    \n",
    "    y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))\n",
    "  \n",
    "\n",
    "def custom_acc_orig_non_other_tokens(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  calculate loss dfunction explicitly filtering out also the 'Other'- labels\n",
    "\n",
    "  y_true: Shape: (batch x (max_length) )\n",
    "  y_pred: predictions. Shape: (batch x x (max_length + 1) x num_distinct_ner_tokens ) \n",
    "\n",
    "  returns: accuracy\n",
    "  \"\"\"\n",
    "\n",
    "  #get labels and predictions\n",
    "\n",
    "  y_label = tf.reshape(tf.layers.Flatten()(tf.cast(y_true, tf.int64)),[-1])\n",
    "\n",
    "  mask = (y_label < 1)\n",
    "  y_label_masked = tf.boolean_mask(y_label, mask)\n",
    "\n",
    "  y_predicted = tf.math.argmax(input = tf.reshape(tf.layers.Flatten()(tf.cast(y_pred, tf.float64)),\\\n",
    "                                                  [-1, 5]), axis=1)\n",
    "\n",
    "  y_predicted_masked = tf.boolean_mask(y_predicted, mask)\n",
    "\n",
    "  return tf.reduce_mean(tf.cast(tf.equal(y_predicted_masked,y_label_masked) , dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QDUg8CH0A9Gq"
   },
   "source": [
    "#### Split data into datasets for training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4brhZ81h_mj2"
   },
   "outputs": [],
   "source": [
    "train_size = round(0.8 * len(input_ids))\n",
    "\n",
    "dev_end = train_size + round(0.1 * train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_P_KcEyKCXr-"
   },
   "outputs": [],
   "source": [
    "shuffle = np.random.permutation(len(input_ids))\n",
    "\n",
    "input_ids = input_ids[shuffle]\n",
    "input_masks = input_masks[shuffle]\n",
    "segment_ids = segment_ids[shuffle]\n",
    "ner_labels = segment_ids[shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOFzsA0xBbJF"
   },
   "outputs": [],
   "source": [
    "train_input_ids, train_input_masks, train_segment_ids =   input_ids[:train_size], input_masks[:train_size], segment_ids[:train_size]\n",
    "dev_input_ids, dev_input_masks, dev_segment_ids =   input_ids[train_size:dev_end], input_masks[train_size:dev_end], segment_ids[train_size:dev_end]\n",
    "test_input_ids, test_input_masks, test_segment_ids =   input_ids[dev_end:], input_masks[dev_end:], segment_ids[dev_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DzK_emvRGl7s"
   },
   "outputs": [],
   "source": [
    "train_labels = ner_labels[:train_size]\n",
    "dev_labels = ner_labels[train_size:dev_end]\n",
    "test_labels = ner_labels[dev_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LSdhnOZUE6ac"
   },
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0oQ8spsC-Cy"
   },
   "outputs": [],
   "source": [
    "\n",
    "def initialize_session(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cSmLi_9IFDvH"
   },
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Create BERT layer, following https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
    "    init:  initialize layer. Specify various parameters regarding output types and dimensions. Very important is\n",
    "           to set the number of trainable layers.\n",
    "    build: build the layer based on parameters\n",
    "    call:  call the BERT layer within a model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"sequence\",\n",
    "        bert_url=\"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_url = bert_url\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_url, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "        trainable_layers = []\n",
    "\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "        mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzY1iSpoDq3L"
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "af6M2kWMEZBr"
   },
   "outputs": [],
   "source": [
    "def ner_model(max_input_length, train_layers):\n",
    "    \"\"\"\n",
    "    Implementation of NER model\n",
    "    \n",
    "    variables:\n",
    "        max_input_length: number of tokens (max_length + 1)\n",
    "        train_layers: number of layers to be retrained\n",
    "        optimizer: optimizer to be used\n",
    "    \n",
    "    returns: model\n",
    "    \"\"\"\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), name=\"segment_ids\")\n",
    "    \n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_sequence = BertLayer(n_fine_tune_layers=train_layers)(bert_inputs)\n",
    "    \n",
    "    print(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(21, activation='softmax', name='ner')(dense)\n",
    "     \n",
    "    print('pred: ', pred)\n",
    "    \n",
    "    ## Prepare for multipe loss functions, although not used here\n",
    "    \n",
    "    losses = {\n",
    "        \"ner\": custom_loss,\n",
    "        }\n",
    "    lossWeights = {\"ner\": 1.0\n",
    "                  }\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    \n",
    "    #model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[custom_acc_orig_tokens])\n",
    "\n",
    "    model.compile(loss=losses, optimizer='adam', metrics=[custom_acc_orig_tokens, \n",
    "                                                         custom_acc_orig_non_other_tokens])\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "id": "9RQCsXg9EeNu",
    "outputId": "de51765d-2bb2-451d-a92c-5da6d4248298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bert_layer_4/bert_layer_4_module_apply_tokens/bert/encoder/Reshape_13:0\", shape=(?, ?, 768), dtype=float32)\n",
      "pred:  Tensor(\"ner_4/truediv:0\", shape=(?, ?, 21), dtype=float32)\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_4 (BertLayer)        (None, None, 768)    108931396   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 256)    196864      bert_layer_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, None, 256)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ner (Dense)                     (None, None, 21)     5397        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,133,657\n",
      "Trainable params: 28,553,749\n",
      "Non-trainable params: 80,579,908\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ner_model(max_len, train_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fWFznN51JxIH"
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "initialize_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "zebG4jo7E1gW",
    "outputId": "931eb8fc-bbca-4dfc-d448-6db00eef9cf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9226 samples, validate on 923 samples\n",
      "Epoch 1/10\n",
      "9226/9226 [==============================] - 20s 2ms/sample - loss: 1.6481 - custom_acc_orig_tokens: 0.1986 - custom_acc_orig_non_other_tokens: 0.1986 - val_loss: 1.6095 - val_custom_acc_orig_tokens: 0.2374 - val_custom_acc_orig_non_other_tokens: 0.2374\n",
      "Epoch 2/10\n",
      "9226/9226 [==============================] - 11s 1ms/sample - loss: 1.6094 - custom_acc_orig_tokens: 0.2839 - custom_acc_orig_non_other_tokens: 0.2839 - val_loss: 1.6094 - val_custom_acc_orig_tokens: 0.2862 - val_custom_acc_orig_non_other_tokens: 0.2862\n",
      "Epoch 3/10\n",
      "9226/9226 [==============================] - 11s 1ms/sample - loss: 1.6094 - custom_acc_orig_tokens: 0.2860 - custom_acc_orig_non_other_tokens: 0.2860 - val_loss: 1.6094 - val_custom_acc_orig_tokens: 0.2384 - val_custom_acc_orig_non_other_tokens: 0.2384\n",
      "Epoch 4/10\n",
      "9226/9226 [==============================] - 11s 1ms/sample - loss: 1.6094 - custom_acc_orig_tokens: 0.2501 - custom_acc_orig_non_other_tokens: 0.2501 - val_loss: 1.6094 - val_custom_acc_orig_tokens: 0.2862 - val_custom_acc_orig_non_other_tokens: 0.2862\n",
      "Epoch 5/10\n",
      "9226/9226 [==============================] - 11s 1ms/sample - loss: 1.6094 - custom_acc_orig_tokens: 0.2820 - custom_acc_orig_non_other_tokens: 0.2820 - val_loss: 1.6094 - val_custom_acc_orig_tokens: 0.2860 - val_custom_acc_orig_non_other_tokens: 0.2860\n",
      "Epoch 6/10\n",
      "9226/9226 [==============================] - 11s 1ms/sample - loss: 1.6094 - custom_acc_orig_tokens: 0.2753 - custom_acc_orig_non_other_tokens: 0.2753 - val_loss: 1.6094 - val_custom_acc_orig_tokens: 0.2860 - val_custom_acc_orig_non_other_tokens: 0.2860\n",
      "Epoch 7/10\n",
      "9226/9226 [==============================] - 11s 1ms/sample - loss: 1.6094 - custom_acc_orig_tokens: 0.2514 - custom_acc_orig_non_other_tokens: 0.2514 - val_loss: 1.6094 - val_custom_acc_orig_tokens: 0.2383 - val_custom_acc_orig_non_other_tokens: 0.2383\n",
      "Epoch 8/10\n",
      "9226/9226 [==============================] - 11s 1ms/sample - loss: 1.6094 - custom_acc_orig_tokens: 0.2468 - custom_acc_orig_non_other_tokens: 0.2468 - val_loss: 1.6094 - val_custom_acc_orig_tokens: 0.1906 - val_custom_acc_orig_non_other_tokens: 0.1906\n",
      "Epoch 9/10\n",
      "9226/9226 [==============================] - 11s 1ms/sample - loss: 1.6094 - custom_acc_orig_tokens: 0.2409 - custom_acc_orig_non_other_tokens: 0.2409 - val_loss: 1.6094 - val_custom_acc_orig_tokens: 0.1906 - val_custom_acc_orig_non_other_tokens: 0.1906\n",
      "Epoch 10/10\n",
      "9226/9226 [==============================] - 11s 1ms/sample - loss: 1.6094 - custom_acc_orig_tokens: 0.2390 - custom_acc_orig_non_other_tokens: 0.2390 - val_loss: 1.6094 - val_custom_acc_orig_tokens: 0.1922 - val_custom_acc_orig_non_other_tokens: 0.1922\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    {\"ner\": train_labels },\n",
    "    validation_data=([dev_input_ids, dev_input_masks, dev_segment_ids], {\"ner\": dev_labels }),\n",
    "    epochs=10,\n",
    "    batch_size=128\n",
    "    #callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mRNo4XViHggz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NERBERT.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
