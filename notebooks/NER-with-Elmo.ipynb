{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "from keras.layers import Layer\n",
    "from keras import backend as K\n",
    "import keras.layers as layers\n",
    "from keras.models import Model, load_model\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tibit Communications Raises $20M in Series B F...</td>\n",
       "      <td>Tibit Communications, Inc., a Petaluma, CA-bas...</td>\n",
       "      <td>http://www.finsmes.com/2019/04/tibit-communica...</td>\n",
       "      <td>FinsmesUSA</td>\n",
       "      <td>Funding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter blames human error after blocking a Ne...</td>\n",
       "      <td>Over the holiday weekend, The New York Times f...</td>\n",
       "      <td>https://techcrunch.com/2017/11/27/twitter-blam...</td>\n",
       "      <td>techcrunch</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SimplyCook Raises £4.5M in Series A Funding\\n</td>\n",
       "      <td>SimplyCook, a London, UK-based recipe kit serv...</td>\n",
       "      <td>http://www.finsmes.com/2019/01/simplycook-rais...</td>\n",
       "      <td>FinsmesUK</td>\n",
       "      <td>Funding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moogsoft Secures $40M in Series D Funding\\n</td>\n",
       "      <td>Moogsoft, a San Francisco, CA-based provider o...</td>\n",
       "      <td>http://www.finsmes.com/2018/03/moogsoft-secure...</td>\n",
       "      <td>FinsmesUSA</td>\n",
       "      <td>Funding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zeta Global acquires commenting service†Disqus</td>\n",
       "      <td>A source close to the two companies tells us t...</td>\n",
       "      <td>https://techcrunch.com/2017/12/05/zeta-global-...</td>\n",
       "      <td>techcrunch</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Tibit Communications Raises $20M in Series B F...   \n",
       "1  Twitter blames human error after blocking a Ne...   \n",
       "2      SimplyCook Raises £4.5M in Series A Funding\\n   \n",
       "3        Moogsoft Secures $40M in Series D Funding\\n   \n",
       "4     Zeta Global acquires commenting service†Disqus   \n",
       "\n",
       "                                             content  \\\n",
       "0  Tibit Communications, Inc., a Petaluma, CA-bas...   \n",
       "1  Over the holiday weekend, The New York Times f...   \n",
       "2  SimplyCook, a London, UK-based recipe kit serv...   \n",
       "3  Moogsoft, a San Francisco, CA-based provider o...   \n",
       "4  A source close to the two companies tells us t...   \n",
       "\n",
       "                                                link      source    class  \n",
       "0  http://www.finsmes.com/2019/04/tibit-communica...  FinsmesUSA  Funding  \n",
       "1  https://techcrunch.com/2017/11/27/twitter-blam...  techcrunch    Other  \n",
       "2  http://www.finsmes.com/2019/01/simplycook-rais...   FinsmesUK  Funding  \n",
       "3  http://www.finsmes.com/2018/03/moogsoft-secure...  FinsmesUSA  Funding  \n",
       "4  https://techcrunch.com/2017/12/05/zeta-global-...  techcrunch    Other  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset/data/articles_dataset.csv')\n",
    "data = data[~data['content'].isnull()]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entities(text):\n",
    "    doc = nlp(text)\n",
    "    organizations = [ent.text for ent in doc.ents if ent.label_=='ORG']\n",
    "    moneys = [ent.text for ent in doc.ents if ent.label_=='MONEY']\n",
    "    people = [ent.text for ent in doc.ents if ent.label_=='PERSON']\n",
    "    \n",
    "    for org in organizations:\n",
    "        text = text.replace(org, 'organization')\n",
    "        \n",
    "    for money in moneys:\n",
    "        text = text.replace(money, 'money')\n",
    "        \n",
    "    for person in people:\n",
    "        text = text.replace(person, 'person')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>class</th>\n",
       "      <th>modified_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tibit Communications Raises $20M in Series B F...</td>\n",
       "      <td>Tibit Communications, Inc., a Petaluma, CA-bas...</td>\n",
       "      <td>http://www.finsmes.com/2019/04/tibit-communica...</td>\n",
       "      <td>FinsmesUSA</td>\n",
       "      <td>Funding</td>\n",
       "      <td>organization, a Petaluma, organization-based s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter blames human error after blocking a Ne...</td>\n",
       "      <td>Over the holiday weekend, The New York Times f...</td>\n",
       "      <td>https://techcrunch.com/2017/11/27/twitter-blam...</td>\n",
       "      <td>techcrunch</td>\n",
       "      <td>Other</td>\n",
       "      <td>Over the holiday weekend, organization found t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SimplyCook Raises £4.5M in Series A Funding\\n</td>\n",
       "      <td>SimplyCook, a London, UK-based recipe kit serv...</td>\n",
       "      <td>http://www.finsmes.com/2019/01/simplycook-rais...</td>\n",
       "      <td>FinsmesUK</td>\n",
       "      <td>Funding</td>\n",
       "      <td>organization, a London, UK-based recipe kit se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moogsoft Secures $40M in Series D Funding\\n</td>\n",
       "      <td>Moogsoft, a San Francisco, CA-based provider o...</td>\n",
       "      <td>http://www.finsmes.com/2018/03/moogsoft-secure...</td>\n",
       "      <td>FinsmesUSA</td>\n",
       "      <td>Funding</td>\n",
       "      <td>organization, a San Francisco, organization-ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zeta Global acquires commenting service†Disqus</td>\n",
       "      <td>A source close to the two companies tells us t...</td>\n",
       "      <td>https://techcrunch.com/2017/12/05/zeta-global-...</td>\n",
       "      <td>techcrunch</td>\n",
       "      <td>Other</td>\n",
       "      <td>A source close to the two companies tells us t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Tibit Communications Raises $20M in Series B F...   \n",
       "1  Twitter blames human error after blocking a Ne...   \n",
       "2      SimplyCook Raises £4.5M in Series A Funding\\n   \n",
       "3        Moogsoft Secures $40M in Series D Funding\\n   \n",
       "4     Zeta Global acquires commenting service†Disqus   \n",
       "\n",
       "                                             content  \\\n",
       "0  Tibit Communications, Inc., a Petaluma, CA-bas...   \n",
       "1  Over the holiday weekend, The New York Times f...   \n",
       "2  SimplyCook, a London, UK-based recipe kit serv...   \n",
       "3  Moogsoft, a San Francisco, CA-based provider o...   \n",
       "4  A source close to the two companies tells us t...   \n",
       "\n",
       "                                                link      source    class  \\\n",
       "0  http://www.finsmes.com/2019/04/tibit-communica...  FinsmesUSA  Funding   \n",
       "1  https://techcrunch.com/2017/11/27/twitter-blam...  techcrunch    Other   \n",
       "2  http://www.finsmes.com/2019/01/simplycook-rais...   FinsmesUK  Funding   \n",
       "3  http://www.finsmes.com/2018/03/moogsoft-secure...  FinsmesUSA  Funding   \n",
       "4  https://techcrunch.com/2017/12/05/zeta-global-...  techcrunch    Other   \n",
       "\n",
       "                                    modified_content  \n",
       "0  organization, a Petaluma, organization-based s...  \n",
       "1  Over the holiday weekend, organization found t...  \n",
       "2  organization, a London, UK-based recipe kit se...  \n",
       "3  organization, a San Francisco, organization-ba...  \n",
       "4  A source close to the two companies tells us t...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['modified_content'] = data['content'].map(replace_entities)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('dataset/data/ner_articles_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = data['modified_content'].values\n",
    "titles = data['title'].values\n",
    "labels = data['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "\n",
    "def encode_labels(labels):\n",
    "    encoded = label_encoder.transform(labels)\n",
    "    return encoded\n",
    "\n",
    "def decode_labels(encoded_labels):\n",
    "    labels_type = type(encoded_labels)\n",
    "    \n",
    "    if labels_type == int:\n",
    "        try:\n",
    "            label = label_encoder.inverse_transform([encoded_labels])\n",
    "        except ValueError:\n",
    "            print('Unknown value')\n",
    "            return np.nan\n",
    "        \n",
    "        return label\n",
    "    \n",
    "    elif hasattr(encoded_labels, '__iter__') and labels_type != str:\n",
    "        try:\n",
    "            labels = label_encoder.inverse_transform(encoded_labels)\n",
    "            \n",
    "        except ValueError:\n",
    "            print('Unknown value')\n",
    "            raise\n",
    "            \n",
    "        return labels\n",
    "    else:\n",
    "        raise TypeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce articles to 150 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = np.array([' '.join(article.split()[:150]) for article in articles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training, dev and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35225 4403 39628\n"
     ]
    }
   ],
   "source": [
    "data_size = len(articles)\n",
    "\n",
    "train_size = round(0.8 * data_size)\n",
    "dev_size = round(0.1 * data_size)\n",
    "\n",
    "dev_end = train_size + dev_size\n",
    "\n",
    "print(train_size, dev_size, dev_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = np.random.permutation(len(articles))\n",
    "articles = articles[shuffle]\n",
    "titles = titles[shuffle]\n",
    "labels = labels[shuffle]\n",
    "\n",
    "articles_train, labels_train, titles_train = articles[:train_size], labels[:train_size], titles[:train_size]\n",
    "articles_dev, labels_dev, titles_dev = articles[train_size:dev_end], labels[train_size:dev_end], titles[train_size:dev_end]\n",
    "articles_test, labels_test, titles_test = articles[dev_end:], labels[dev_end:], titles[dev_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "#### Create Elmo Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmoEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.dimensions = 1024\n",
    "        self.trainable=True\n",
    "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n",
    "                               name=\"{}_module\".format(self.name))\n",
    "\n",
    "        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
    "                      as_dict=True,\n",
    "                      signature='default',\n",
    "                      )['elmo']\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 48, self.dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    input_text = layers.Input(shape=(1,), dtype=\"string\")\n",
    "    embedding = ElmoEmbeddingLayer()(input_text)\n",
    "    lstm = layers.Bidirectional(layers.LSTM(512))(embedding)\n",
    "    dense = layers.Dense(128, activation='relu')(lstm)\n",
    "    pred = layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = Model(inputs=[input_text], outputs=pred)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "elmo_embedding_layer_2 (Elmo (None, 48, 1024)          4         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1024)              6295552   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 6,426,885\n",
      "Trainable params: 6,426,885\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Early Stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0803 20:54:11.576054 140196501894976 deprecation_wrapper.py:119] From /home/brianmusisi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35225 samples, validate on 4403 samples\n",
      "Epoch 1/10\n",
      "35225/35225 [==============================] - 409s 12ms/step - loss: 0.2976 - acc: 0.8798 - val_loss: 0.2413 - val_acc: 0.9126\n",
      "Epoch 2/10\n",
      "35225/35225 [==============================] - 373s 11ms/step - loss: 0.2090 - acc: 0.9199 - val_loss: 0.2227 - val_acc: 0.9135\n",
      "Epoch 3/10\n",
      "35225/35225 [==============================] - 372s 11ms/step - loss: 0.1711 - acc: 0.9336 - val_loss: 0.2442 - val_acc: 0.9121\n",
      "Epoch 4/10\n",
      "35225/35225 [==============================] - 372s 11ms/step - loss: 0.1247 - acc: 0.9512 - val_loss: 0.2812 - val_acc: 0.9096\n",
      "Epoch 5/10\n",
      "35225/35225 [==============================] - 370s 11ms/step - loss: 0.0775 - acc: 0.9712 - val_loss: 0.3729 - val_acc: 0.9076\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Build and fit\n",
    "model.reset_states()\n",
    "history = model.fit(articles_train, labels_train,\n",
    "                  validation_data=(articles_dev, labels_dev),\n",
    "                    epochs=10, batch_size=128, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.9155\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(articles_test, labels_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/data/ner_articles_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = data['modified_content'].values\n",
    "titles = data['title'].values\n",
    "labels = data['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = np.array([' '.join(article.split()[:150]) for article in articles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35225 4403 39628\n"
     ]
    }
   ],
   "source": [
    "data_size = len(articles)\n",
    "\n",
    "train_size = round(0.8 * data_size)\n",
    "dev_size = round(0.1 * data_size)\n",
    "\n",
    "dev_end = train_size + dev_size\n",
    "\n",
    "print(train_size, dev_size, dev_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = np.random.permutation(len(articles))\n",
    "articles = articles[shuffle]\n",
    "titles = titles[shuffle]\n",
    "labels = labels[shuffle]\n",
    "\n",
    "articles_train, labels_train, titles_train = articles[:train_size], labels[:train_size], titles[:train_size]\n",
    "articles_dev, labels_dev, titles_dev = articles[train_size:dev_end], labels[train_size:dev_end], titles[train_size:dev_end]\n",
    "articles_test, labels_test, titles_test = articles[dev_end:], labels[dev_end:], titles[dev_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmoEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.dimensions = 1024\n",
    "        self.trainable=True\n",
    "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n",
    "                               name=\"{}_module\".format(self.name))\n",
    "\n",
    "        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
    "                      as_dict=True,\n",
    "                      signature='default',\n",
    "                      )['elmo']\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 48, self.dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    input_text = layers.Input(shape=(1,), dtype=\"string\")\n",
    "    embedding = ElmoEmbeddingLayer()(input_text)\n",
    "    lstm = layers.LSTM(512)(embedding)\n",
    "    dense = layers.Dense(30, activation='relu')(lstm)\n",
    "    pred = layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = Model(inputs=[input_text], outputs=pred)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0803 23:04:44.124125 139833800501056 deprecation_wrapper.py:119] From /home/brianmusisi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0803 23:04:44.127584 139833800501056 deprecation_wrapper.py:119] From /home/brianmusisi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0803 23:04:45.351426 139833800501056 deprecation_wrapper.py:119] From /home/brianmusisi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0803 23:04:46.483433 139833800501056 deprecation_wrapper.py:119] From /home/brianmusisi/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0803 23:04:46.505125 139833800501056 deprecation_wrapper.py:119] From /home/brianmusisi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0803 23:04:46.510905 139833800501056 deprecation.py:323] From /home/brianmusisi/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "elmo_embedding_layer_1 (Elmo (None, 48, 1024)          4         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 512)               3147776   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                15390     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 3,163,201\n",
      "Trainable params: 3,163,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Early Stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0803 23:05:17.745397 139833800501056 deprecation_wrapper.py:119] From /home/brianmusisi/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35225 samples, validate on 4403 samples\n",
      "Epoch 1/10\n",
      "35225/35225 [==============================] - 330s 9ms/step - loss: 0.2969 - acc: 0.8798 - val_loss: 0.2320 - val_acc: 0.9146\n",
      "Epoch 2/10\n",
      "35225/35225 [==============================] - 324s 9ms/step - loss: 0.2145 - acc: 0.9191 - val_loss: 0.2248 - val_acc: 0.9171\n",
      "Epoch 3/10\n",
      "35225/35225 [==============================] - 323s 9ms/step - loss: 0.1900 - acc: 0.9267 - val_loss: 0.2320 - val_acc: 0.9210\n",
      "Epoch 4/10\n",
      "35225/35225 [==============================] - 323s 9ms/step - loss: 0.1620 - acc: 0.9375 - val_loss: 0.2479 - val_acc: 0.9146\n",
      "Epoch 5/10\n",
      "35225/35225 [==============================] - 323s 9ms/step - loss: 0.1276 - acc: 0.9504 - val_loss: 0.2707 - val_acc: 0.9123\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Build and fit\n",
    "model.reset_states()\n",
    "history = model.fit(articles_train, labels_train,\n",
    "                  validation_data=(articles_dev, labels_dev),\n",
    "                    epochs=10, batch_size=128, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.9121\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(articles_test, labels_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
